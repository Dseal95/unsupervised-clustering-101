{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:GREEN\">**DS 101 - Unsupervised Learning (Clustering)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: \n",
    "Apply **K-Means()** and **DBSCAN()** clustering algorithms to synthetic and real data. \n",
    "\n",
    "- **DBSCAN()** Original Paper - https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf \n",
    "- ***K-Means()*** Original Paper - https://digitalassets.lib.berkeley.edu/math/ucb/text/math_s5_v1_article-17.pdf\n",
    "\n",
    "## Aims:\n",
    "   > 1. To be able to apply **Kmeans()** and **DBSCAN()** algorithms to a dataset.\n",
    "   >\n",
    "   >\n",
    "   >\n",
    "   > 2. Understand the advantages and drawbacks of both clustering algorithms on different datasets.\n",
    "   >\n",
    "   >\n",
    "   >\n",
    "   > 3. Apply and understand 2 clustering metrics that can be used to optimise / tune the algorithm parameters\n",
    "   >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you are in the right DIR\n",
    "\n",
    "!ls -l  # Mac\n",
    "# %dir # windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Installing Modules...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from layouts import layout_2d, scene_3d\n",
    "from matplotlib import pyplot as plt\n",
    "from model import AutoDBSCAN, EpsTuner\n",
    "from plotting import (\n",
    "    generate_elbow_plots,\n",
    "    plot_2d_clustered_scatter,\n",
    "    plot_2d_scatter,\n",
    "    plot_3d_clustered_scatter,\n",
    "    plot_3d_scatter,\n",
    "    plot_null_heat_map,\n",
    "    build_plot_layout,\n",
    ")\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score\n",
    "from utils import (\n",
    "    euclidean_dist_2d,\n",
    "    euclidean_dist_3d,\n",
    "    generate_2d_synthetic_blob_data,\n",
    "    generate_3d_synthetic_blob_data,\n",
    "    join_data,\n",
    "    scaler,\n",
    ")\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed to make NB reproducible\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:BLUE\">**1. 2-D Synthetic Data Generation**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Notebook Only</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate synthetic 2 dimenational data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) generate 2D raw data + plot\n",
    "x, y = generate_2d_synthetic_blob_data([1, 5], [0.5, 0.5], 1000, add_noise=False)\n",
    "\n",
    "# create 2d dataset\n",
    "X = pd.DataFrame(data=list(zip(x, y)), columns=[\"x\", \"y\"])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot raw data (no labels)\n",
    "plot_2d_scatter(x, y, layout_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot raw data (with labels)\n",
    "plot_2d_scatter(x, y, layout_2d, labels=[0] * 1000 + [1] * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    ">\n",
    "> - If you are able to (2d or 3d), always plot the raw data and get a sense of what you perceive the clusters to be. For partitioning clustering this will allow you to input an educated guess for the number of clusters and for hierarchical clustering, it will enable you to move towards better model parameters more quickly. \n",
    ">\n",
    ">\n",
    "> - Inability to view data > 3d = **Curse of dimensionality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">**2. Clustering in 2-D**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><u>*Bookmark*: Slides 6 (K-Means), 15, 16, 17 and 18 (DBSCAN)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Clustering the **2 dimensional data** using:\n",
    ">\n",
    ">    - **KMeans**(*n_clusters*)\n",
    ">    - **DBSCAN**(*min_samples*, *eps*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">1. K-Means(n_clusters) Clustering 2-D</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit KMeans() to data with n_clusters=2\n",
    "model = KMeans(n_clusters=2).fit(X)\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(X, model)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clustered data\n",
    "plot_2d_clustered_scatter(df=df, layout=layout_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Silhouette Coefficient: {silhouette_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calinski Harabasz Score: {calinski_harabasz_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">2. DBSCAN(eps, min_samples) Clustering in 2-D</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit DBSCAN() to data with default eps and min_samples as suggested >= n-dim + 2\n",
    "model = DBSCAN(eps=1, min_samples=4).fit(X)\n",
    "\n",
    "# create a dataset with the raw features and the label\n",
    "df = join_data(X, model)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clustered data\n",
    "plot_2d_clustered_scatter(df=df, layout=layout_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Silhouette Coefficient: {silhouette_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calinski Harabasz Score: {calinski_harabasz_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    ">\n",
    "> - Both **K-Means()** and **DBSCAN()** were able to successfully cluster the synthetic 2-d gausssian blobs. \n",
    "> - The synthetic data that we have been clustered is just gaussian blobs with **no noise**. In real life, it very unlikely that your data will be devoid of noise / outliers.\n",
    "> - In the next section we will **introduce noise in 3D** and assess how well the clustering techniques perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:BLUE\">**3. 3-D Synthetic Data Generation with Noise**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Notebook Only</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3D raw data + plot\n",
    "x, y, z = generate_3d_synthetic_blob_data(\n",
    "    [0, 3, -3], [0.75, 0.4, 0.5], 1000, add_noise=True\n",
    ")\n",
    "\n",
    "# create 3d dataset\n",
    "X = pd.DataFrame(data=list(zip(x, y, z)), columns=[\"x\", \"y\", \"z\"])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_3d_scatter(x, y, z, layout=scene_3d)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:BLUE\">**4. Clustering in 3-D**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><u>*Bookmark*: Slides 6 (K-Means), 15, 16, 17 and 18 (DBSCAN)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Clustering the **3 dimensional data with added noise** using:\n",
    ">\n",
    ">    - **K-Means**(*n_clusters*)\n",
    ">    - **DBSCAN**(*min_samples*, *eps*)\n",
    ">\n",
    ">\n",
    ">\n",
    ">\n",
    "*Q. Should the noise be clustered?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">1. K-Means(n_clusters) Clustering 3-D (with noise)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit KMeans() to data with n_clusters=3\n",
    "model = KMeans(n_clusters=3).fit(X)\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(X, model)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clustered data\n",
    "plot_3d_clustered_scatter(df=df, layout=scene_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Clusters = {len(set(model.labels_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Silhouette Coefficient: {silhouette_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calinski Harabasz Score: {calinski_harabasz_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">1. DBSCAN(eps, min_samples) Clustering 3-D (with noise)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit DBSCAN() to data with default eps and min_samples as suggested >= n-dim + 2\n",
    "model = DBSCAN(eps=0.5, min_samples=6).fit(X)\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(X, model)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clustered data\n",
    "plot_3d_clustered_scatter(df=df, layout=scene_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Clusters = {len(set(model.labels_)) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Silhouette Coefficient: {silhouette_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calinski Harabasz Score: {calinski_harabasz_score(X, model.labels_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - **DBSCAN()** is able to decifer between manually added noise and the actual clusters. **K-Means()** is not. This is because **K-Means()** is an partitioning algorithm that introduces a decision boundary to ditinguish between the clusters. It is **exhaustive**. DBSCAN does not introduce a decision boundary as it is based on **density connectivity**, allowing it to not cluster sparce points.\n",
    ">\n",
    ">\n",
    "> - Noise, if actually outliers **should NOT** be clustered. \n",
    "> \n",
    ">\n",
    "> - Even though **DBSCAN()** is able to filter out the noise, the **Silhouette coefficient** and **Calinski Harabasz Score** of **K-Means()** are higher.\n",
    ">\n",
    ">\n",
    "> - A drawback of both of **Silhouette Coefficient** and **Calinski Harabasz Score** is that they are generally higher for convex clusters than other concepts of clusters (arbitrary shape), such as density based clusters like those obtained through **DBSCAN()**. However, it should be noted that the non-clustered data points are being included in the metric calculations.\n",
    ">\n",
    ">\n",
    "\n",
    "\n",
    "**Q. If we remove the noise from the calculation of the Silhouette Coefficient and Calinski Harabasz Score, does DBSCAN perform better?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Removing the non-clustered data points before computing the metric...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clustered data with noise filtered out\n",
    "plot_3d_clustered_scatter(df=df[df.cluster != -1], layout=scene_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the noise labelled data points from the data and the labels\n",
    "data_noise_removed = X[model.labels_ != -1]\n",
    "labels_noise_removed = model.labels_[model.labels_ != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Silhouette Coefficient: {silhouette_score(data_noise_removed, labels_noise_removed)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Calinski Harabasz Score: {calinski_harabasz_score(data_noise_removed, labels_noise_removed)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - Removing the noise has improved the **Silhouette Coefficient** and **Calinski Harabasz** Score for **DBSCAN**().\n",
    ">\n",
    ">\n",
    "> - According to the metrics, **DBSCAN**() is now performing better than **K-Means**().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Type            | Silhouette          |Calinski Harabasz   |\n",
    "| --------------------- | ------------------- |--------------------|\n",
    "| **DBSCAN**() No Noise | 0.76288807009818    | 30717.096411445338 |\n",
    "|                       |                     |                    |\n",
    "| **KMeans**() w/ Noise | 0.719957008289208   | 17490.405259058403 |\n",
    "|                       |                     |                    |\n",
    "| **DBSCAN**() w/ Noise | 0.680839999833934   | 9826.981463540193  |\n",
    "|                       |                     |                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Black\">**Q: How To Choose the Best Model Hyperparameters / What is the optimal number of clusters?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">**5. Threshold Tuning Using Performance Metrics (Synthetic Data)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Slides **23**, **24** and **25**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder on the performance metrics...*\n",
    "\n",
    "> - **Silhouette Coefficient** = The silhouette value is a measure of how similar an object is to its own cluster compared to other clusters. Silhouette coefficients near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster. *A greater cluster separation = higher score*.\n",
    "> \n",
    ">\n",
    "> - **Calinski Harabasz** = The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion. The score is higher when clusters are dense and well separated.\n",
    "\n",
    "\n",
    "**Note:** Silhouette Coefficient and Calinski Harabasz do not need ground truth labels to be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">1. K-Means(n_clusters)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **K-Means()** only has one tunable parameter, *n_clusters*.\n",
    ">\n",
    ">\n",
    "> - In order to find the optimal number of clusters we will vary *n_clusters* and assess the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of n_clusters that we want to assess\n",
    "n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "S_scores = []\n",
    "CH_scores = []\n",
    "\n",
    "for i in n_clusters:\n",
    "    model = KMeans(n_clusters=i).fit(X)\n",
    "    S_scores.append(silhouette_score(X, model.labels_))\n",
    "    CH_scores.append(calinski_harabasz_score(X, model.labels_))\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    data=list(zip(n_clusters, S_scores, CH_scores)),\n",
    "    columns=[\"n_clusters\", \"silhouette\", \"calinski_harabasz\"],\n",
    ")\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_fig, ch_fig = generate_elbow_plots(\n",
    "    df_metrics,\n",
    "    layout=build_plot_layout(\n",
    "        title=\"Silhouette score / CH Score as a function of varying clusters.\",\n",
    "        x_axis_title=\"n_clusters\",\n",
    "        y_axis_title=\"Silhouette / CH Score\",\n",
    "        h=500,\n",
    "        w=500,\n",
    "    ),\n",
    "    model_type=\"kmeans\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_fig.add_vline(x=3, line_width=1.5, line_dash=\"dash\", line_color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_fig.add_vline(x=3, line_width=1.5, line_dash=\"dash\", line_color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">2. DBSCAN(eps, min_samples)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **DBSCAN()** has two tunable parameters, **eps** and **min_samples**.\n",
    ">\n",
    ">\n",
    "> - **eps** is the most important hyperparameter (arguably) but we will demonstrate a good way of tuning it automatically.\n",
    ">\n",
    ">\n",
    "> - We are going to fix min_samples to be > n-dimensions + 1 and then assess on a varying set of eps values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing varying n_clusters on the metrics\n",
    "MinPts = 5  # >= dim + 2\n",
    "\n",
    "S_scores = []\n",
    "CH_scores = []\n",
    "S_scores_no_noise = []\n",
    "CH_scores_no_noise = []\n",
    "noise = []\n",
    "num_clusters = []\n",
    "Eps = [\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.2,\n",
    "    0.25,\n",
    "    0.3,\n",
    "    0.35,\n",
    "    0.4,\n",
    "    0.45,\n",
    "    0.5,\n",
    "    0.55,\n",
    "    0.6,\n",
    "    0.65,\n",
    "    0.7,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.9,\n",
    "    1.0,\n",
    "    1.1,\n",
    "    1.2,\n",
    "    1.25,\n",
    "    1.3,\n",
    "    1.4,\n",
    "    1.5,\n",
    "]\n",
    "\n",
    "for eps in Eps:\n",
    "    print(eps)\n",
    "    model = DBSCAN(min_samples=MinPts, eps=eps).fit(X)\n",
    "\n",
    "    S_scores.append(silhouette_score(X, model.labels_))\n",
    "    CH_scores.append(calinski_harabasz_score(X, model.labels_))\n",
    "\n",
    "    # applying metrics to the data with no noise\n",
    "    S_scores_no_noise.append(\n",
    "        silhouette_score(X[model.labels_ != -1], model.labels_[model.labels_ != -1])\n",
    "    )\n",
    "    CH_scores_no_noise.append(\n",
    "        calinski_harabasz_score(\n",
    "            X[model.labels_ != -1], model.labels_[model.labels_ != -1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    noise.append(len(X[model.labels_ == -1]))\n",
    "    num_clusters.append(len(set(model.labels_)) - (1 if -1 in model.labels_ else 0))\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    data=list(\n",
    "        zip(\n",
    "            Eps,\n",
    "            S_scores,\n",
    "            CH_scores,\n",
    "            S_scores_no_noise,\n",
    "            CH_scores_no_noise,\n",
    "            num_clusters,\n",
    "            noise,\n",
    "        )\n",
    "    ),\n",
    "    columns=[\n",
    "        \"eps\",\n",
    "        \"silhouette_noise\",\n",
    "        \"calinski_harabasz_noise\",\n",
    "        \"silhouette\",\n",
    "        \"calinski_harabasz\",\n",
    "        \"number_of_clusters\",\n",
    "        \"noise\",\n",
    "    ],\n",
    ")\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the figures of the metrics\n",
    "silhouette_fig, ch_fig = generate_elbow_plots(\n",
    "    df_metrics,\n",
    "    layout=build_plot_layout(\n",
    "        title=f\"Silhouette score / CH Score as a function of varying eps values.\",\n",
    "        x_axis_title=\"eps\",\n",
    "        y_axis_title=\"Silhouette / CH Score\",\n",
    "        h=700,\n",
    "        w=700,\n",
    "    ),\n",
    "    model_type=\"dbscan\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_fig.add_vline(x=0.45, line_width=1.5, line_dash=\"dash\", line_color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_fig\n",
    "ch_fig.add_vline(x=0.45, line_width=1.5, line_dash=\"dash\", line_color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">**6. Threshold Tuning Using Distances (Sythnetic Data)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">1. KMeans(n_clusters)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Notebook Only</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - You can choose the optimal number of *n_clusters* using the cluster inerta and assessing this across a number of different n_clusters. \n",
    ">\n",
    ">\n",
    "> - **Inertia** = Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if  provided.\n",
    ">\n",
    ">\n",
    "> - The basic assumption is that \"best\" clustered datasets will have the lowest inertia as the cluster samples will all be nearest to their clustere centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning using inertia\n",
    "n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "inertias = []\n",
    "for i in n_clusters:\n",
    "    model = KMeans(n_clusters=i).fit(X)\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "# plot the inertia as a function of varying n_clusters\n",
    "fig = go.Figure(\n",
    "    data=go.Scatter(x=n_clusters, y=inertias),\n",
    "    layout=build_plot_layout(\n",
    "        title=\"Cluster Inertia Score (intra-cluster distance) for varying n_clusters.\",\n",
    "        x_axis_title=\"n_clusters\",\n",
    "        y_axis_title=\"Cluster Inertia Score\",\n",
    "        h=800,\n",
    "        w=800,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_vline(x=3, line_width=1.5, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">2. AutoDBSCAN(eps, min_samples)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Slides **Appendix (2)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Similar to **K-Means()** above, you can optimally select the eps value for **DBSCAN()** using **NearestNeighbours**. By taking the **Kth (min_samples + 1)** distance for each data point and **plotting the sorted distances**, you should get an **elbow**. \n",
    ">\n",
    ">\n",
    "> - Data points in dense areas (likely to be clustered) will have very similar values of Kth dist in comparison to data points in sparce regions where their values will exponentiate. The elbow is the point at which you maximise the eps-neighbourhood whilst minimising the number of outliers you have in the final clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoDBSCAN(min_samples=5).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = go.Figure(\n",
    "    data=go.Scatter(x=np.arange(len(model.knn_distances)), y=model.knn_distances),\n",
    "    layout=go.Layout(\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"Kth Neighbour distance (eps approximation)\",\n",
    "        yaxis=dict(\n",
    "            gridcolor=\"lightgrey\",\n",
    "            showgrid=True,\n",
    "            showline=True,\n",
    "            linecolor=\"black\",\n",
    "            linewidth=2,\n",
    "            mirror=True,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            gridcolor=\"lightgrey\",\n",
    "            showgrid=True,\n",
    "            showline=True,\n",
    "            linecolor=\"black\",\n",
    "            linewidth=2,\n",
    "            dtick=500,\n",
    "            mirror=True,\n",
    "        ),\n",
    "        plot_bgcolor=\"rgb(255, 255, 255)\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig2.add_hline(y=model.eps, line_width=1.5, line_dash=\"dash\", line_color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of clusters = {len(set(model.clustering.labels_)) - (1 if -1 in model.clustering.labels_ else 0)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_arr = np.bincount(model.clustering.labels_[model.clustering.labels_ != -1])\n",
    "print(\n",
    "    f\"cluster data points (cluster number, data points): {list(zip([0, 1, 2, 3, 4], count_arr))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"AutoDBSCAN() number of noise points = {len(model.clustering.labels_[model.clustering.labels_ == -1])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit DBSCAN() to data with auto selected eps and min_samples as suggested > n-dim + 1\n",
    "model = DBSCAN(eps=0.40509132373679185, min_samples=5).fit(X)\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(X, model)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Silhouette Coefficient: {silhouette_score(X[model.labels_ != -1], model.labels_[model.labels_ != -1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Calinski Harabasz Score: {calinski_harabasz_score(X[model.labels_ != -1], model.labels_[model.labels_ != -1])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clustered data\n",
    "plot_3d_clustered_scatter(df=df[df.cluster != -1], layout=scene_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - **K-Means Optimising:** The optimum number of clusters for **K-Means()** = **3** according to the performance metrics and the elbow in the inertia curve plot. This configuration returns:\n",
    "       - Silhouette Score = 0.719957\n",
    "       - Calinski-Harabaz Score = 17490.405259   \n",
    ">\n",
    ">\n",
    "> - **DBSCAN Optimising:** The Optimium eps for **DBSCAN()** = **0.45** according to the metrics returning:\n",
    "       - Silhouette Score = 0.764421\n",
    "       - Calinski-Harabaz Score = 31060.399601\n",
    "       - Number of noise points = 189\n",
    ">\n",
    "> \n",
    "> - **AutoDBSCAN Optimising:** was able to identify 219 noise points with an auto selected eps=**0.40509132373679185**. When generating the synthetic 3-D data we added ~ 100 noise data points and so we have managed to successfully capture most of the noise. This configuration returns:\n",
    "       - Silhouette Score = 0.7671902118407119\n",
    "       - Calinski-Harabaz Score = 31721.846726745272\n",
    "       - Number of noise points = 219\n",
    ">\n",
    ">\n",
    ">\n",
    "> DBSCAN outperforms K-Means on 3D synthetic data with noise!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">**7. Scaling**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Notebook Only (Scaling is not in the PowerPoint)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:black\">**Q: Do I Need to SCALE my data before clustering?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:black\">*A note on scaling the data...*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  - The **general rule of thumb** on scaling when clustering is to **consider the data**. Standardising (scaling) the data will give **equal weights to all of the clustered features**. If this is something that you would like to do then scale, otherwise omit it from your clustering pipeline. The reason for this is a feature with a significantly higher order of magnitude to the other features will be the primary driver of distance. \n",
    ">\n",
    ">\n",
    ">  - If you have mixed numerical data, where each attribute is something entirely different (say, shoe size and weight), has different units attached (lb, tons, m, kg ...) then these values aren't really comparable anyway; z-standardizing them is a best-practise to give equal weight to them\n",
    ">\n",
    ">\n",
    "> - The z-standard score of a sample x is calculated as: **z = (x - u) / s**\n",
    ">\n",
    ">\n",
    "> - \"A primary application of geometrical measures (distances) to features having large ranges will implicitly assign greater efforts in the metrics compared to the application with features having smaller ranges. Furthermore, the features need to be dimensionless since the numerical values of the ranges of dimensional features rely upon the units of measurements and, hence, a selection of the units of measurements may significantly alter the outcomes of clustering. Therefore, one should not employ distance measures like the Euclidean distance without having normalization of the data sets (Aksoy and Haralick, 2001; Larose, 2005).\"\n",
    "\n",
    "> LINK - https://pdfs.semanticscholar.org/6666/ebfbfc39cadd9a273c9a2f1ad8150644ac20.pdf?_ga=2.103320393.812854306.1642607291-1097112116.1642607291\n",
    "\n",
    ">\n",
    "> **Consider the following Qs**:\n",
    "> - **(1)** If you are clustering people based on their weights (kgs) and height (m), is a 1kg difference in weight the same as a metre difference in height?\n",
    ">\n",
    ">\n",
    "> - **(2)** Does it matter that you would get different clusterings on weights (kgs) and heights (cm)? \n",
    "\n",
    "If your answers are **\"no\"** and **\"yes\"** respectively then *you should probably scale*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">Height and Weight Example</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"data/height_weight.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data and apply some conversions to get the columns that we want\n",
    "hw = pd.read_csv(\"data/height_weight.csv\")\n",
    "\n",
    "# drop index column\n",
    "hw = hw.drop([\"Index\"], axis=1)\n",
    "\n",
    "# apply some basic unit conversions\n",
    "hw[\"Height(m)\"] = hw[\"Height(Inches)\"] * 0.0254\n",
    "hw[\"Height(cm)\"] = hw[\"Height(m)\"] * 100\n",
    "hw[\"Weight(kg)\"] = hw[\"Weight(Pounds)\"] * 0.453592\n",
    "\n",
    "hw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the raw data:\n",
    "\n",
    "\n",
    "fig = plot_2d_scatter(\n",
    "    x=hw[\"Height(m)\"].to_list(),\n",
    "    y=hw[\"Weight(kg)\"].to_list(),\n",
    "    layout=build_plot_layout(\n",
    "        title=\"height versus weight\",\n",
    "        x_axis_title=\"height (m)\",\n",
    "        y_axis_title=\"weight (kg)\",\n",
    "        h=600,\n",
    "        w=600,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "proportionality_line = {\n",
    "    \"type\": \"line\",\n",
    "    \"line\": {\"color\": \"red\", \"dash\": \"dash\"},\n",
    "    \"yref\": \"paper\",\n",
    "    \"xref\": \"paper\",\n",
    "    \"y0\": 0,\n",
    "    \"y1\": 1,\n",
    "    \"x0\": 0,\n",
    "    \"x1\": 1,\n",
    "}\n",
    "\n",
    "fig.update_layout(shapes=[proportionality_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">1. Clustering Raw Data (No Scaling):</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_2d = layout_2d.update({\"xaxis\": dict(dtick=0.05)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit KMeans() to data with n_clusters=2 and scaled\n",
    "model = KMeans(n_clusters=2).fit(hw[[\"Height(m)\", \"Weight(kg)\"]])\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(hw[[\"Height(m)\", \"Weight(kg)\"]], model)\n",
    "\n",
    "# plotting the clustered data\n",
    "fig1 = plot_2d_clustered_scatter(\n",
    "    df=df,\n",
    "    layout=build_plot_layout(\n",
    "        title=\"clustered height (m) versus weight (kg)\",\n",
    "        x_axis_title=\"height (m)\",\n",
    "        y_axis_title=\"weight (kg)\",\n",
    "        h=600,\n",
    "        w=600,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig1.update_layout(shapes=[proportionality_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - weight (kg) is an order of magnitude higher than ~ 10 times higher than height (m)\n",
    ">\n",
    ">\n",
    "> - the y-axis (weight) is therefore the primary driver when KMeans() computes the euclidean distance to assign data points to clusters\n",
    ">\n",
    ">\n",
    "> - The bias in the y-axis has meant that the clustering has been ~ done by cutting the y-axis in half. This doesnt lend itself to the linear relationship between weight and height.\n",
    ">\n",
    ">\n",
    "> - we would expect that the clutering separate the \"2 clusters\" with a diagnonal line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">2. Clustering Data with unit conversion applied to acquire similar magnitudes:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_2d = layout_2d.update({\"xaxis\": dict(dtick=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit KMeans() to data with n_clusters=2 and scaled\n",
    "model = KMeans(n_clusters=2).fit(hw[[\"Height(cm)\", \"Weight(kg)\"]])\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(hw[[\"Height(cm)\", \"Weight(kg)\"]], model)\n",
    "\n",
    "# plotting the clustered data\n",
    "fig2 = plot_2d_clustered_scatter(\n",
    "    df=df,\n",
    "    layout=build_plot_layout(\n",
    "        title=\"clustered height (cm) versus weight (kg) - unit conversion applied\",\n",
    "        x_axis_title=\"height (cm)\",\n",
    "        y_axis_title=\"weight (kg)\",\n",
    "        h=600,\n",
    "        w=600,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig2.update_layout(shapes=[proportionality_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">3. Clustering when z-standardisation applied to raw data:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit KMeans() to data with n_clusters=2 and scaled\n",
    "\n",
    "hw_scaled = scaler(\n",
    "    hw[[\"Height(m)\", \"Weight(kg)\"]], hw[[\"Height(m)\", \"Weight(kg)\"]].columns\n",
    ")\n",
    "\n",
    "model = KMeans(n_clusters=2).fit(hw_scaled)\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(hw[[\"Height(m)\", \"Weight(kg)\"]], model)\n",
    "\n",
    "# plotting the clustered data\n",
    "fig3 = plot_2d_clustered_scatter(\n",
    "    df=df,\n",
    "    layout=build_plot_layout(\n",
    "        title=\"clustered height (cm) versus weight (kg) - z-standardisation applied\",\n",
    "        x_axis_title=\"height (cm)\",\n",
    "        y_axis_title=\"weight (kg)\",\n",
    "        h=600,\n",
    "        w=600,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig3.update_layout(shapes=[proportionality_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - Due to the synthetic data having similar magnitudes, scaling has not been required.\n",
    ">\n",
    ">\n",
    "> - Repeating the clustering on scaled data should return ~ same results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "\n",
    "xs = [10000, 22000, 15000, 10000]\n",
    "ys = [55, 90, 75, 55]\n",
    "zs = [1.5, 4, 9, 1.5]\n",
    "raw = pd.DataFrame(data=list(zip(xs, ys, zs)), columns=[\"x\", \"y\", \"z\"])\n",
    "\n",
    "raw_data = go.Scatter3d(\n",
    "    x=xs,\n",
    "    y=ys,\n",
    "    z=zs,\n",
    "    mode=\"markers+lines+text\",\n",
    "    textposition=\"top center\",\n",
    "    text=[str(i) for i in list(zip(xs, ys, zs))],\n",
    "    showlegend=False,\n",
    "    opacity=0.5,\n",
    "    marker=dict(color=\"black\", size=2),\n",
    "    hovertemplate=\"<b>value:</b><br>\" + \"x: %{x}<br>\" + \"y: %{y}<br>\" + \"z: %{z}<br>\",\n",
    ")\n",
    "\n",
    "fig = go.Figure([raw_data])\n",
    "fig.update_scenes(scene_3d)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\n",
    "    f\"euclidean between raw c0 and c1 = {euclidean_dist_3d(raw.iloc[0].to_list(), raw.iloc[1].to_list())}\"\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\n",
    "    f\"euclidean between raw c1 and c2 = {euclidean_dist_3d(raw.iloc[1].to_list(), raw.iloc[2].to_list())}\"\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\n",
    "    f\"euclidean between raw c0 and c2 = {euclidean_dist_3d(raw.iloc[0].to_list(), raw.iloc[2].to_list())}\"\n",
    ")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scaled = scaler(raw, list(raw.columns))\n",
    "\n",
    "scaled_data = go.Scatter3d(\n",
    "    x=np.array(raw_scaled.x).ravel(),\n",
    "    y=np.array(raw_scaled.y).ravel(),\n",
    "    z=np.array(raw_scaled.z).ravel(),\n",
    "    mode=\"markers+lines+text\",\n",
    "    textposition=\"top center\",\n",
    "    text=[\n",
    "        str(i)\n",
    "        for i in list(\n",
    "            zip(\n",
    "                [round(i, 1) for i in np.array(raw_scaled.x).ravel()],\n",
    "                [round(i, 1) for i in np.array(raw_scaled.y).ravel()],\n",
    "                [round(i, 1) for i in np.array(raw_scaled.z).ravel()],\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    showlegend=False,\n",
    "    opacity=0.5,\n",
    "    marker=dict(color=\"black\", size=2),\n",
    "    hovertemplate=\"<b>value:</b><br>\" + \"x: %{x}<br>\" + \"y: %{y}<br>\" + \"z: %{z}<br>\",\n",
    ")\n",
    "\n",
    "fig = go.Figure([scaled_data])\n",
    "fig.update_scenes(scene_3d)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\n",
    "    f\"euclidean between scaled c0 and c1 = {euclidean_dist_3d(raw_scaled.iloc[0].to_list(), raw_scaled.iloc[1].to_list())}\"\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\n",
    "    f\"euclidean between scaled c1 and c2 = {euclidean_dist_3d(raw_scaled.iloc[1].to_list(), raw_scaled.iloc[2].to_list())}\"\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\n",
    "    f\"euclidean between scaled c0 and c2 = {euclidean_dist_3d(raw_scaled.iloc[0].to_list(), raw_scaled.iloc[2].to_list())}\"\n",
    ")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - The magnitudes of the euclidean distances when using the raw data are of the order of the x feature's magnitude. The x feature is driving the distance. However, when we z-standardise the data, the magnitudes of the distances are within the order of all of the scaled variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:BLUE\">**8. Clustering on REAL Data**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering on real data: (basic)\n",
    "\n",
    "> **1.** **Data Cleansing**: Subset to numerical variables \n",
    ">\n",
    "> **2.** **Data Cleansing**: Remove rows that contain ANY NaNs \n",
    ">\n",
    "> **3.** **Data Exloration**: (Optional) Look at distributions of variables \n",
    ">\n",
    "> **4.** **Data Exloration**: Plot the raw data (if able to) and get an estimate for the number of clusters by eye\n",
    ">\n",
    "> **5.** **Data Preparation**: z-standardise the data (equally weight variables if applicable)\n",
    ">\n",
    "> **6.** **Cluster**: Cluster the data, finding the optimum value through eye (if not cursed by dimentionality) and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Purple\">1. Data Cleansing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into a pandas df\n",
    "df = pd.read_csv(\"data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to label and 4 numerical features\n",
    "df = df[\n",
    "    [\n",
    "        \"Species\",\n",
    "        \"Culmen Length (mm)\",\n",
    "        \"Culmen Depth (mm)\",\n",
    "        \"Flipper Length (mm)\",\n",
    "        \"Body Mass (g)\",\n",
    "    ]\n",
    "]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN Heatmap\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "sns.heatmap(~df.isnull(), cbar=False, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with any NaNs\n",
    "df_clean = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN Heatmap\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "sns.heatmap(~df_clean.isnull(), cbar=False, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, i in enumerate(list(df_clean.Species.unique())):  # 3 penguin types\n",
    "    print(f\"class {e} = {i}, count = {df_clean[df_clean.Species == i].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Class Imbalance - 1 is about 50% under represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Purple\">2. Data Exploration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions of features\n",
    "h1 = go.Histogram(x=df_clean[\"Culmen Length (mm)\"], opacity=1, name=\"Culmen Length\")\n",
    "h2 = go.Histogram(x=df_clean[\"Culmen Depth (mm)\"], opacity=1, name=\"Culmen Depth\")\n",
    "h3 = go.Histogram(x=df_clean[\"Flipper Length (mm)\"], opacity=0.6, name=\"Flipper Length\")\n",
    "# h4 = go.Histogram(x=df_clean[\"Body Mass (g)\"], opacity=0.6, name='Body Mass')\n",
    "\n",
    "go.Figure(data=[h1, h2, h3], layout=go.Layout(barmode=\"overlay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_3d.update(\n",
    "    {\n",
    "        \"xaxis_title\": \"Culmen Length (mm)\",\n",
    "        \"yaxis_title\": \"Culmen Depth (mm)\",\n",
    "        \"zaxis_title\": \"Flipper Length (mm)\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 3D data\n",
    "plot_3d_scatter(\n",
    "    x=df_clean[\"Culmen Length (mm)\"],\n",
    "    y=df_clean[\"Culmen Depth (mm)\"],\n",
    "    z=df_clean[\"Flipper Length (mm)\"],\n",
    "    layout=scene_3d,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - Looking at the raw data, both 2 and 3 seem like good guessed for the number of clusters.\n",
    ">\n",
    ">\n",
    "> - We know that there are 3 classes, will optimising using metrics be able to finalise on a model, either KMeans() or DBSCAN() that can produce 3 cluster?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using Silhouette Score, CH Score and Inerta, what is the optimum number of clusters? Does this reflect the true labels?\n",
    "\n",
    "\n",
    "--- initial guess is 2/3? 3 after looking more closely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training data (removing the label)\n",
    "X = df_clean[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\"]]\n",
    "\n",
    "# scaled data using z-standardisation\n",
    "X_scaled = scaler(X, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Slides **23**, **24** and **25**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">3. Clustering: (a) K-Means(n_clusters)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of n_clusters that we want to assess\n",
    "n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "# metrics\n",
    "S_scores = []\n",
    "CH_scores = []\n",
    "inertias = []\n",
    "\n",
    "for i in n_clusters:\n",
    "    model = KMeans(n_clusters=i).fit(X_scaled)\n",
    "    S_scores.append(silhouette_score(X_scaled, model.labels_))\n",
    "    CH_scores.append(calinski_harabasz_score(X_scaled, model.labels_))\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    data=list(zip(n_clusters, S_scores, CH_scores, inertias)),\n",
    "    columns=[\"n_clusters\", \"silhouette\", \"calinski_harabasz\", \"inertias\"],\n",
    ")\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette coefficient\n",
    "m1 = go.Scatter(x=n_clusters, y=S_scores, name=\"silhouette coefficient\")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[m1],\n",
    "    layout=build_plot_layout(\n",
    "        title=\"Silhouette score as a function of varying clusters.\",\n",
    "        x_axis_title=\"n_clusters\",\n",
    "        y_axis_title=\"Silhouette\",\n",
    "        h=500,\n",
    "        w=500,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_vline(x=2, line_width=1.5, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CH score\n",
    "m2 = go.Scatter(x=n_clusters, y=CH_scores, name=\"calinski_harabasz\")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[m2],\n",
    "    layout=build_plot_layout(\n",
    "        title=\"CH Score as a function of varying clusters.\",\n",
    "        x_axis_title=\"n_clusters\",\n",
    "        y_axis_title=\"CH Score\",\n",
    "        h=500,\n",
    "        w=500,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_vline(x=3, line_width=1.5, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inertias\n",
    "m3 = go.Scatter(x=n_clusters, y=inertias, name=\"cluster inertia\")\n",
    "fig = go.Figure(\n",
    "    data=[m3],\n",
    "    layout=build_plot_layout(\n",
    "        title=\"Cluster inertia score as a function of varying clusters.\",\n",
    "        x_axis_title=\"n_clusters\",\n",
    "        y_axis_title=\"cluster inertia Score\",\n",
    "        h=500,\n",
    "        w=500,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_vline(\n",
    "    x=3, line_width=1.5, line_dash=\"dash\", line_color=\"red\"\n",
    ")  # 3, 4, 5 - the elbow is not clear\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3).fit(X_scaled)\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(X, model)\n",
    "\n",
    "plot_3d_clustered_scatter(df, layout=scene_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">3. Clustering: (b) DBSCAN(eps, min_samples)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing varying n_clusters on the metrics\n",
    "MinPts = 6  # > dim + 2\n",
    "\n",
    "S_scores = []\n",
    "CH_scores = []\n",
    "S_scores_no_noise = []\n",
    "CH_scores_no_noise = []\n",
    "noise = []\n",
    "pct_noise = []\n",
    "num_clusters = []\n",
    "Eps = []\n",
    "\n",
    "for eps in [\n",
    "    0.2,\n",
    "    0.23,\n",
    "    0.26,\n",
    "    0.3,\n",
    "    0.33,\n",
    "    0.36,\n",
    "    0.4,\n",
    "    0.43,\n",
    "    0.46,\n",
    "    0.47,\n",
    "    0.48,\n",
    "    0.5,\n",
    "    0.53,\n",
    "    0.56,\n",
    "    0.6,\n",
    "    0.63,\n",
    "    0.66,\n",
    "    0.7,\n",
    "]:\n",
    "    model = DBSCAN(min_samples=MinPts, eps=eps).fit(X_scaled)\n",
    "\n",
    "    print(f\"cluster set = {set(model.labels_)}, eps = {eps}, MinPts = {MinPts}\")\n",
    "\n",
    "    Eps.append(eps)\n",
    "    S_scores.append(silhouette_score(X_scaled, model.labels_))\n",
    "    CH_scores.append(calinski_harabasz_score(X_scaled, model.labels_))\n",
    "    S_scores_no_noise.append(\n",
    "        silhouette_score(\n",
    "            X_scaled[model.labels_ != -1], model.labels_[model.labels_ != -1]\n",
    "        )\n",
    "    )\n",
    "    CH_scores_no_noise.append(\n",
    "        calinski_harabasz_score(\n",
    "            X_scaled[model.labels_ != -1], model.labels_[model.labels_ != -1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    noise.append(len(X_scaled[model.labels_ == -1]))\n",
    "    pct_noise.append((len(X_scaled[model.labels_ == -1]) / len(X_scaled)) * 100)\n",
    "\n",
    "    num_clusters.append(len(set(model.labels_)) - 1)\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    data=list(\n",
    "        zip(\n",
    "            Eps,\n",
    "            S_scores,\n",
    "            CH_scores,\n",
    "            S_scores_no_noise,\n",
    "            CH_scores_no_noise,\n",
    "            num_clusters,\n",
    "            noise,\n",
    "            pct_noise,\n",
    "        )\n",
    "    ),\n",
    "    columns=[\n",
    "        \"eps\",\n",
    "        \"silhouette\",\n",
    "        \"calinski_harabasz\",\n",
    "        \"silhouette_no_noise\",\n",
    "        \"calinski_harabasz_no_noise\",\n",
    "        \"number_of_clusters\",\n",
    "        \"noise\",\n",
    "        \"% noise\",\n",
    "    ],\n",
    ")\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset is ~ 350 data points large so make sure not to choose an eps values that treats the bulk of the data as noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette coefficient\n",
    "m1a = go.Scatter(\n",
    "    x=Eps[6:],\n",
    "    y=S_scores[6:],\n",
    "    name=\"silhouette coefficient\",\n",
    "    text=[str(n) for n in num_clusters[cut_off:]],\n",
    "    textposition=\"top center\",\n",
    "    mode=\"markers+lines+text\",\n",
    "    opacity=0.5,\n",
    ")\n",
    "\n",
    "m1b = go.Scatter(\n",
    "    x=Eps[6:],\n",
    "    y=S_scores_no_noise[6:],\n",
    "    name=\"silhouette coefficient no noise\",\n",
    "    text=[str(n) for n in num_clusters[cut_off:]],\n",
    "    textposition=\"top center\",\n",
    "    mode=\"markers+lines+text\",\n",
    "    opacity=1,\n",
    ")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[m1a, m1b],\n",
    "    layout=build_plot_layout(\n",
    "        title=f\"Silhouette score for varying formed clusters annotated with eps value. MinPts={MinPts}.\",\n",
    "        x_axis_title=\"eps\",\n",
    "        y_axis_title=\"Silhouette\",\n",
    "        h=800,\n",
    "        w=800,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_vline(x=0.4, line_width=1.5, line_dash=\"dash\", line_color=\"green\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CH coefficient\n",
    "m2a = go.Scatter(\n",
    "    x=Eps[6:],\n",
    "    y=CH_scores[6:],\n",
    "    name=\"calinski_harabasz\",\n",
    "    text=[str(n) for n in num_clusters[cut_off:]],\n",
    "    textposition=\"top center\",\n",
    "    mode=\"markers+lines+text\",\n",
    "    opacity=0.5,\n",
    ")\n",
    "\n",
    "m2b = go.Scatter(\n",
    "    x=Eps[6:],\n",
    "    y=CH_scores_no_noise[6:],\n",
    "    name=\"calinski_harabasz_no_noise\",\n",
    "    text=[str(n) for n in num_clusters[cut_off:]],\n",
    "    textposition=\"top center\",\n",
    "    mode=\"markers+lines+text\",\n",
    ")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[m2a, m2b],\n",
    "    layout=build_plot_layout(\n",
    "        title=f\"CH score for varying formed clusters annotated with eps value. MinPts={MinPts}.\",\n",
    "        x_axis_title=\"eps\",\n",
    "        y_axis_title=\"CH Score\",\n",
    "        h=800,\n",
    "        w=800,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_vline(x=0.4, line_width=1.5, line_dash=\"dash\", line_color=\"green\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(eps=0.4, min_samples=6).fit(X_scaled)\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "df = join_data(X, model)\n",
    "\n",
    "plot_3d_clustered_scatter(df, layout=scene_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clustered_scatter(df[df.cluster != -1], layout=scene_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">3. Clustering: (c) AutoDBSCAN(min_samples)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Orange\"><u>*Bookmark*: Appendix (2)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoDBSCAN(min_samples=6).fit(X_scaled)\n",
    "print(f\"auto-selected eps value = {model.eps}\")\n",
    "\n",
    "# create a dataset with the raw features and the labels\n",
    "cluster_labels = pd.DataFrame(\n",
    "    data=model.clustering.labels_, index=X.index, columns=[\"cluster\"]\n",
    ")\n",
    "df = X.merge(cluster_labels, how=\"inner\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Silhouette Coefficient: {silhouette_score(df[df.cluster != -1], model.clustering.labels_[model.clustering.labels_ != -1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Calinski Harabasz Score: {calinski_harabasz_score(df[df.cluster != -1], model.clustering.labels_[model.clustering.labels_ != -1])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clustered_scatter(df, layout=scene_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "> - The metrics / distance; **Silhouette coefficient**, **calinski_harabasz** and distances did not come to a conclusive number of clusters for the penguins dataset for either **K-Means()** or **DBSCAN()**. It was a battle between 2 and 3.\n",
    ">\n",
    ">\n",
    "> - This is common and it lends itself to the idea that clustering is a difficult and not binary machine learning method (when you do not have true labels). It is a great too for data exploration. \n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remarks:\n",
    "\n",
    "> - This notebook has been focused on 2 and 3d clustering. The reason for that is the **curse of dimensionality** and wanting to have the ability to plot our data and clustered data. It is common for datasets to have > 3 dimensions, and in these cases we need to rely on metrics.\n",
    ">\n",
    ">\n",
    "> - Class imbalances in datasets can affect the clustering algorithms ability to cluster \"properly\"\n",
    ">\n",
    ">\n",
    "> - Chosen to avoid datasets that work well for **K-Means()** and not for **DBSCAN()** and vice versa to help simulate the real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            END OF SCRIPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
